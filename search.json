[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "World Athletics Database as a CSV File",
    "section": "",
    "text": "1 About\nSome 400k athlete performances for various events scraped from World Athletics."
  },
  {
    "objectID": "index.html#get-the-data",
    "href": "index.html#get-the-data",
    "title": "World Athletics Database as a CSV File",
    "section": "1.1 Get the data",
    "text": "1.1 Get the data\nCopy the code below to download the full data set.\nimport pandas as pd\n\npd.read_csv(\n    \"https://raw.githubusercontent.com/thomascamminady/world-athletics-database/main/data/data.csv\",\n    delimiter=\";\",\n    parse_dates=True\n)\nOr find the source code on Github."
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "World Athletics Database as a CSV File",
    "section": "1.2 Development",
    "text": "1.2 Development\nFirst, clone the repository and navigate to the project directory. Then, install the necessary dependencies:\nmake init\nNext, you can parse and post-process the data:\npoetry run python world_athletics_database/parse.py\npoetry run python world_athletics_database/postprocess.py\nNote: Parsing the data may take up to 20 minutes."
  },
  {
    "objectID": "analytics.html",
    "href": "analytics.html",
    "title": "2  A first dive into the data",
    "section": "",
    "text": "Let’s play around with the data a little bit. We will load the data and look at the top 10 marathon performances of all times.\n\n\nCode\nimport polars as pl\nimport altair as alt\nfrom camminapy.plot import altair_theme\n\n\naltair_theme()\ndf = (\n    pl.read_csv(\"data/data.csv\", separator=\";\")\n    .filter(pl.col(\"Event\") == \"Marathon\")\n    .groupby(\"Sex\")\n    .head(10)\n    .with_columns(Hours=pl.col(\"Mark [meters or seconds]\") / 3600)\n)\n\nchart = (\n    alt.Chart(\n        df.with_columns(\n            pl.col(\"Competitor\").str.split(\" \").list.last().alias(\"Last Name\")\n        )\n    )\n    .mark_line(point=True)\n    .encode(\n        x=\"Rank:N\",\n        y=alt.Y(\"Hours:Q\").scale(zero=False),\n        color=alt.Color(\"Sex:N\")\n        .scale(domain=[\"female\", \"male\"], range=[\"purple\", \"orange\"])\n        .title(None)\n        .legend(None),\n        tooltip=[f\"{c}:N\" for c in df.columns],\n    )\n    .properties(width=650, height=300)\n    # .interactive()\n)\nalt.layer(\n    chart, chart.mark_text(dy=-10, dx=10, angle=320).encode(text=\"Last Name:N\")\n).facet(column=\"Sex:N\").resolve_scale(y=\"independent\")\n\n\n\n\n\n\n\n\nWe can also look at how world records have progressed over time.\n\n\nCode\nwr = (\n    pl.read_csv(\"data/data.csv\", separator=\";\", try_parse_dates=True)\n    .sort(\"Sex\", \"Event\", \"Date\")\n    .with_columns((pl.col(\"Mark [meters or seconds]\") / 3600).alias(\"Hours\"))\n    .with_columns(\n        (pl.col(\"Hours\") == pl.col(\"Hours\").cummin())\n        .over(\"Sex\", \"Event\")\n        .alias(\"Performance has been a WR\")\n    )\n    .filter(pl.col(\"Performance has been a WR\"))\n    .filter(pl.col(\"Event\") == \"Marathon\")\n    .with_columns(pl.col(\"Competitor\").str.split(\" \").list.last().alias(\"Last Name\"))\n    .sort(\"Sex\", \"Hours\")\n)\n\n\nchart = (\n    alt.Chart(wr.to_pandas())\n    .mark_line(point=True, interpolate=\"step-after\")\n    .encode(\n        x=\"Date:T\",\n        y=alt.Y(\"Hours:Q\").scale(zero=False),\n        color=alt.Color(\"Sex:N\")\n        .scale(domain=[\"female\", \"male\"], range=[\"purple\", \"orange\"])\n        .title(None)\n        .legend(None),\n        tooltip=[f\"{c}:N\" for c in df.columns],\n    )\n    .properties(width=650, height=300)\n    # .interactive()\n)\nalt.layer(\n    chart, chart.mark_text(dy=-10, dx=10, angle=320).encode(text=\"Last Name:N\")\n).facet(column=\"Sex:N\").resolve_scale(y=\"independent\")"
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "3  Hello, it’s me!",
    "section": "",
    "text": "Hey! It’s me, Thomas. I scraped this data from World Athletics.\nI have a couple of other data sets that you can check out.\nIf you like what I do, check out my website. Cheers!"
  }
]